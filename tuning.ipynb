{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Using WandDB Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from models import CNNStudent, CNNTeacher, Resnet18, Resnet34, Resnet50\n",
    "import wandb\n",
    "import argparse\n",
    "from time import perf_counter\n",
    "\n",
    "# function to extract cifar-10 datasets and apply transformations\n",
    "def get_cifar10_datasets(img_size=32):\n",
    "    train_transforms_cifar = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(0.1),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.75, scale=(0.02, 0.1), value=1.0, inplace=False)\n",
    "    ])\n",
    "\n",
    "    test_transforms_cifar = transforms.Compose([\n",
    "        transforms.Resize((img_size,img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # load and download data\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms_cifar)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms_cifar)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def soft_target_loss(soft_targets, soft_prob, temperature):\n",
    "    return -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (temperature**2)\n",
    "\n",
    "# function to train model for 1 epoch using cross-entropy and soft-target loss\n",
    "def distill_knowledge_ce_stl(teacher, student, optimizer, train_loader, device, temperature, ce_weight, st_weight):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "        with torch.no_grad():\n",
    "            teacher_logits, _ = teacher(inputs)\n",
    "\n",
    "        # Forward pass with the student model\n",
    "        student_logits, _ = student(inputs)\n",
    "\n",
    "        #Soften the student logits by applying softmax first and log() second\n",
    "        soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "        soft_prob = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "\n",
    "        # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "        soft_targets_loss = soft_target_loss(soft_targets, soft_prob, temperature)\n",
    "\n",
    "        # Calculate the true label loss\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "        # Weighted sum of the two losses\n",
    "        loss = st_weight * soft_targets_loss + ce_weight * label_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"batch_train_loss\": loss.item(), \"batch_train\": distill_knowledge_ce_stl.train_batch_counter})\n",
    "        distill_knowledge_ce_stl.train_batch_counter += 1\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def distill_knowledge_ce_csl(teacher, student, optimizer, train_loader, device, ce_weight, cs_weight):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "        with torch.no_grad():\n",
    "            _, teacher_feats = teacher(inputs)\n",
    "\n",
    "        # Forward pass with the student model\n",
    "        student_logits, student_feats = student(inputs)\n",
    "\n",
    "        # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
    "        cosine_loss = nn.CosineEmbeddingLoss()\n",
    "        feats_loss = cosine_loss(student_feats, teacher_feats, target=torch.ones(inputs.size(0)).to(device))\n",
    "\n",
    "        # Calculate the true label loss\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "        # Weighted sum of the two losses\n",
    "        loss = cs_weight * feats_loss + ce_weight * label_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"batch_train_loss\": loss.item(), \"batch_train\": distill_knowledge_ce_csl.train_batch_counter})\n",
    "        distill_knowledge_ce_csl.train_batch_counter += 1\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def distill_knowledge_ce_mse(teacher, student, optimizer, train_loader, device, ce_weight, mse_weight):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "        with torch.no_grad():\n",
    "            _, teacher_feats = teacher(inputs)\n",
    "\n",
    "        # Forward pass with the student model\n",
    "        student_logits, student_feats = student(inputs)\n",
    "\n",
    "        # Calculate the MSE loss. \n",
    "        mse_loss = nn.MSELoss()\n",
    "        feats_loss = mse_loss(student_feats, teacher_feats)\n",
    "\n",
    "        # Calculate the true label loss\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "        # Weighted sum of the two losses\n",
    "        loss = mse_weight * feats_loss + ce_weight * label_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"batch_train_loss\": loss.item(), \"batch_train\": distill_knowledge_ce_mse.train_batch_counter})\n",
    "        distill_knowledge_ce_mse.train_batch_counter += 1\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# function to evaluate model for 1 epoch\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            batch_total = labels.size(0)\n",
    "            batch_correct = (predicted == labels).sum().item()\n",
    "            wandb.log({\"batch_test_acc\": 100 * batch_correct / batch_total, \"batch_test\": test.test_batch_counter})\n",
    "            test.test_batch_counter += 1\n",
    "            total += batch_total\n",
    "            correct += batch_correct\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train():\n",
    "    wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"knowledge-distillation-experiments\",\n",
    "            \n",
    "            # track hyperparameters and run metadata\n",
    "            config=wandb.config\n",
    "        )\n",
    "    \n",
    "    args = wandb.config\n",
    "\n",
    "    if 'resnet' in args.student_mod and 'resnet' in args.teacher_mod:\n",
    "        img_size = 224\n",
    "    elif 'cnn' in args.student_mod and 'cnn' in args.teacher_mod:\n",
    "        img_size = 32\n",
    "\n",
    "    train_data, test_data = get_cifar10_datasets(img_size=img_size)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.train_batch_size, shuffle=True, num_workers=args.train_num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_batch_size, shuffle=False, num_workers=args.test_num_workers)\n",
    "\n",
    "    # initialize student and teacher models\n",
    "    start = perf_counter()\n",
    "    if args.student_mod == 'cnn':\n",
    "        student = CNNStudent(num_classes=10, dropout=args.dropout)\n",
    "    elif args.student_mod == 'resnet18':\n",
    "        student = Resnet18(pretrained=False)\n",
    "    elif args.student_mod == 'resnet18_pt':\n",
    "        student = Resnet18(pretrained=True)\n",
    "\n",
    "    if args.teacher_mod == 'cnn_pt':\n",
    "        teacher = torch.load('./cnn_teacher.pth')\n",
    "    elif args.teacher_mod == 'cnn':\n",
    "        teacher = CNNTeacher(num_classes=10, dropout=args.dropout)\n",
    "    elif args.teacher_mod == 'resnet34':\n",
    "        teacher = Resnet34(pretrained=False)\n",
    "    elif args.teacher_mod == 'resnet34_pt':\n",
    "        teacher = Resnet34(pretrained=True)\n",
    "    elif args.teacher_mod == 'resnet50':\n",
    "        teacher = Resnet50(pretrained=False)\n",
    "    elif args.teacher_mod == 'resnet50_pt':\n",
    "        teacher = Resnet50(pretrained=True)\n",
    "\n",
    "    student = student.to(args.device)\n",
    "    teacher = teacher.to(args.device)\n",
    "    end = perf_counter()\n",
    "    mod_init_time = end - start\n",
    "    wandb.log({'mods_init_time': mod_init_time})\n",
    "\n",
    "    # initialize optimizer\n",
    "    start = perf_counter()\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(student.parameters(), lr=args.lr)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(student.parameters(), lr=args.lr)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(student.parameters(), lr=args.lr)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(student.parameters(), lr=args.lr)\n",
    "    elif args.opt == 'adadelta':\n",
    "        optimizer = optim.Adadelta(student.parameters(), lr=args.lr)\n",
    "    end = perf_counter()\n",
    "    opt_init_time = end - start\n",
    "    wandb.log({'opt_init_time': opt_init_time})\n",
    "\n",
    "    wandb.define_metric(\"batch_train\")\n",
    "    wandb.define_metric(\"batch_test\")\n",
    "    wandb.define_metric(\"epoch\")\n",
    "\n",
    "    wandb.define_metric(\"batch_train_loss\", step_metric=\"batch_train\", summary='last')\n",
    "    wandb.define_metric(\"epoch_train_loss\", step_metric=\"epoch\", summary='last')\n",
    "    wandb.define_metric(\"batch_test_acc\", step_metric=\"batch_test\", summary='last')\n",
    "    wandb.define_metric(\"epoch_test_acc\", step_metric=\"epoch\", summary='last')        \n",
    "\n",
    "    # train and eval mod\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    distill_knowledge_ce_stl.train_batch_counter = 0\n",
    "    distill_knowledge_ce_csl.train_batch_counter = 0\n",
    "    distill_knowledge_ce_mse.train_batch_counter = 0\n",
    "    test.test_batch_counter = 0\n",
    "    for i in range(args.epochs):\n",
    "        if args.loss == 'ce-stl':\n",
    "            train_loss = distill_knowledge_ce_stl(student=student, teacher=teacher, optimizer=optimizer, train_loader=train_loader, device=args.device, ce_weight=args.ce_w, st_weight=args.st_w, temperature=args.temp)\n",
    "        elif args.loss == 'ce-csl':\n",
    "            train_loss = distill_knowledge_ce_csl(teacher=teacher, student=student, optimizer=optimizer, train_loader=train_loader, device=args.device, ce_weight=args.ce_w, cs_weight=args.cs_w)\n",
    "        elif args.loss == 'ce-mse':\n",
    "            train_loss = distill_knowledge_ce_mse(teacher=teacher, student=student, optimizer=optimizer, train_loader=train_loader, device=args.device, ce_weight=args.ce_w, mse_weight=args.mse_w)\n",
    "        test_acc = test(model=student, test_loader=test_loader, device=args.device)\n",
    "        train_losses.append(train_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        wandb.log({\"epoch_train_loss\": train_loss, \"epoch\": i})\n",
    "        wandb.log({\"epoch_test_acc\": test_acc, \"epoch\": i})\n",
    "    \n",
    "    final_test_acc = sum(test_accs) / len(test_accs)\n",
    "    wandb.log({'final_test_acc': final_test_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'final_test_acc'},\n",
      " 'parameters': {'ce_w': {'distribution': 'uniform', 'max': 1, 'min': 0},\n",
      "                'device': {'value': 'cuda'},\n",
      "                'dropout': {'distribution': 'uniform', 'max': 1, 'min': 0.001},\n",
      "                'epochs': {'values': [1, 10, 20]},\n",
      "                'loss': {'values': ['ce-stl']},\n",
      "                'lr': {'distribution': 'uniform', 'max': 1, 'min': 0.0001},\n",
      "                'opt': {'values': ['adam',\n",
      "                                   'sgd',\n",
      "                                   'adagrad',\n",
      "                                   'adadelta',\n",
      "                                   'rmsprop']},\n",
      "                'st_w': {'distribution': 'uniform', 'max': 1, 'min': 0},\n",
      "                'student_mod': {'values': ['cnn']},\n",
      "                'teacher_mod': {'values': ['cnn', 'cnn_pt']},\n",
      "                'temp': {'distribution': 'uniform', 'max': 1, 'min': 0},\n",
      "                'test_batch_size': {'values': [128, 256]},\n",
      "                'test_num_workers': {'values': [2, 4]},\n",
      "                'train_batch_size': {'values': [128, 256]},\n",
      "                'train_num_workers': {'values': [2, 4]}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8pt4a1yv\n",
      "Sweep URL: https://wandb.ai/amanichopra/knowledge-distillation-experiments/sweeps/8pt4a1yv\n"
     ]
    }
   ],
   "source": [
    "sweep_configuration = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"final_test_acc\"},\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {'values': [1, 10, 20]},\n",
    "        'train_num_workers': {'values': [2, 4]},\n",
    "        'test_num_workers': {'values': [2, 4]},\n",
    "        'train_batch_size': {'values': [128, 256]},\n",
    "        'test_batch_size': {'values': [128, 256]},\n",
    "        'lr': {'distribution': 'uniform', \"max\": 1, \"min\": 0.0001},\n",
    "        'dropout': {'distribution': 'uniform', \"max\": 1, \"min\": 0.001},\n",
    "        'loss': {'values': ['ce-stl']},\n",
    "        'opt': {'values': ['adam', 'sgd', 'adagrad', 'adadelta', 'rmsprop']},\n",
    "        'student_mod': {'values': ['cnn']},\n",
    "        'teacher_mod': {'values': ['cnn', 'cnn_pt']},\n",
    "        'ce_w': {'distribution': 'uniform', \"max\": 1, \"min\": 0},\n",
    "        #'cs_w': {'distribution': 'uniform', \"max\": 1, \"min\": 0},\n",
    "        'st_w': {'distribution': 'uniform', \"max\": 1, \"min\": 0},\n",
    "        #'mse_w': {'distribution': 'uniform', \"max\": 1, \"min\": 0},\n",
    "        'temp': {'distribution': 'uniform', \"max\": 1, \"min\": 0},\n",
    "        'device': {'value': 'cuda'}\n",
    "    },\n",
    "}\n",
    "from pprint import pprint\n",
    "pprint(sweep_configuration)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"knowledge-distillation-experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ezdls0oy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tce_w: 0.3579533881775676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdevice: cuda\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.29087420236988465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: ce-stl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.6636154488696536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \topt: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tst_w: 0.4335573999910032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstudent_mod: cnn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_mod: cnn_pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemp: 0.6676902240172179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_num_workers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_num_workers: 4\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamanichopra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amanchopra/knowledge-distillation-experiments/wandb/run-20231216_044550-ezdls0oy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amanichopra/knowledge-distillation-experiments/runs/ezdls0oy' target=\"_blank\">fanciful-sweep-1</a></strong> to <a href='https://wandb.ai/amanichopra/knowledge-distillation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/amanichopra/knowledge-distillation-experiments/sweeps/8pt4a1yv' target=\"_blank\">https://wandb.ai/amanichopra/knowledge-distillation-experiments/sweeps/8pt4a1yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amanichopra/knowledge-distillation-experiments' target=\"_blank\">https://wandb.ai/amanichopra/knowledge-distillation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/amanichopra/knowledge-distillation-experiments/sweeps/8pt4a1yv' target=\"_blank\">https://wandb.ai/amanichopra/knowledge-distillation-experiments/sweeps/8pt4a1yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amanichopra/knowledge-distillation-experiments/runs/ezdls0oy' target=\"_blank\">https://wandb.ai/amanichopra/knowledge-distillation-experiments/runs/ezdls0oy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train, count=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
